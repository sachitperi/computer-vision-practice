# Data Augmentation
Regularization is any modification applied to a learning algorithm that is intended 
to reduce its generalization error but not its training error. Regularisation seeks 
to reduce the testing error at the expense of increasing training error slightly. 

There are various forms of regularization 
1) Parameterized form: requiring us to update our loss/update function 
2) Modifying the network architecture: Dropout is an example of modifying a network 
architecture by achieving greater generalizability. Here we insert a layer that 
randomly disconnects nodes from previous layers to the next layer, there by ensuring 
that no single node is responsible for learning how to represent a given class. 
3) Augment the data passed into the network for training: The method purposefully 
perturbs training examples changing their appearance slightly before passing them 
into the network for training. Thee result is the network consistently sees new training 
data, partially alleviating the need for us to gather more training data. 

In context of computer vision additional training data can be generated by applying 
the following transformations
1) Translations
2) Rotations
3) Changes in scale 
4) Shearing 
5) Horizontal (in some cases vertical flips)  
Applying a small amount of these transformations will change its appearance slightly, 
but does-not change the class label. Other advabced techniques include random 
perturbation of colors in a given color space and non linear geometric distortions. 

A sample demo can be seen using the augmentation_demo.py script. 
It requires two mandatory parameters (other parameters are optional) 
a) path to the input image and b) path to the output directory to store the augmented images.

Sample:    
Input: orriginal_image  
![orriginal_image](../data/input/augmentation/zoro.jpeg)  
output: augmented images  
![orriginal_image](../data/output/augmentation/image_0_199.jpg)
![orriginal_image](../data/output/augmentation/image_0_438.jpg)
![orriginal_image](../data/output/augmentation/image_0_797.jpg)
![orriginal_image](../data/output/augmentation/image_0_2272.jpg)
![orriginal_image](../data/output/augmentation/image_0_5041.jpg)
![orriginal_image](../data/output/augmentation/image_0_5071.jpg)
![orriginal_image](../data/output/augmentation/image_0_5671.jpg)

Data augmentation can help reduce over-fitting. 
When working with datasets that have too few examples to apply deep learning 
data augmentation can be utilized to generate additional data. 

A good way to visualize the same is by 
comparing training with and without data augmentation 
1) Train MiniVGGNet on Flowers-17 without data augmentation 
2) Train MiniVGGNet on Flowers-17 with data augmentation 

## Aspect-aware preprocessing 
Images can be resized by either 
1) ignoring the aspect ratio  
or  
2) maintaining the aspect ratio 

Aspect ratio can be ignored when resizing basic benchmark datasets. But when dealing with 
more challenging datasets aspect ratio needs to be maintained. 
To maintain the ratio we introduce the same in out pre-processing module 
`aspectawarepreprocessing.py`. 
```
|--- utilities  
|   |--- __init__.py  
|   |--- datasets  
|   |   |--- __init__.py  
|   |   |--- simpleedataseetloader.py  
|   |--- preprocessing  
|   |   |--- __init__.py  
|   |   |--- aspectawarepreproceessor.py  
|   |   |--- imagetoarraypreprocessor.py   
|   |   |--- simplepreprocessor.py  
|   |--- nn  
|   |   |--- __init__.py  
|   |   |--- conv  
|   |   |   |--- __init__.py  
|   |   |   |--- shallownet.py  
|   |   |   |--- lenet.py  
|   |--- callbacks  
|   |   |--- __init__.py   
|   |   |--- trainingmonitor.py  
|   |--- utils  
|   |   |--- __init__.py  
|   |   |--- captchahelper.py   
```  